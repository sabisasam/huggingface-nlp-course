{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPpUkNaS6rq1MXEBjidSdck"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Hugging Face NLP Course - Chapter 5**\n","\n","Source: https://huggingface.co/learn/nlp-course/chapter5/1\n","\n","---"],"metadata":{"id":"zHRdgn0b3hss"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlTUxTT93c6h"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"markdown","source":["## **5. The HF Datasets Library**"],"metadata":{"id":"-lfPiqAY3vjz"}},{"cell_type":"markdown","source":["### **5.1. What if my dataset isn't on the Hub?**\n","\n","You know how to use the Hugging Face Hub to download datasets, but you'll often find yourself working with data that is stored either on your laptop or on a remote server. In this section we'll show you how HF Datasets can be used to load datasets that aren't available on the Hugging Face Hub."],"metadata":{"id":"9mXdVePU4qNj"}},{"cell_type":"markdown","source":["#### **Working with local and remote datasets**\n","\n","HF Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n","\n","| Data format | Loading script | Example |\n","|:---:|:---:|:---:|\n","| CSV & TSV | `csv` | `load_dataset(\"csv\", data_files=\"my_file.csv\")` |\n","| Text files | `text` | `load_dataset(\"text\", data_files=\"my_file.txt\")` |\n","| JSON & JSON Lines | `json` | `load_dataset(\"json\", data_files=\"my_file.jsonl\")` |\n","| Parquet | `parquet` | `load_dataset(\"parquet\", data_files=\"my_file.parquet\")` |\n","| Pickled DataFrames | `pandas` | `load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")` |\n","\n","<br />\n","\n","As shown in the table, for each data format we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files` argument that specifies the path to one or more files."],"metadata":{"id":"MlnCEOiSrNL1"}},{"cell_type":"markdown","source":["#### **Loading a local dataset**\n","\n","For this example we'll use the [SQuAD-it dataset](https://github.com/crux82/squad-it/), which is a large-scale dataset for question answering in Italian.\n","\n","The training and test splits are hosted on GitHub, so we can download them with a simple `wget` command:"],"metadata":{"id":"DjORUaL8urGr"}},{"cell_type":"code","source":["!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n","!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"],"metadata":{"id":"CAediF5-v_y1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will download two compressed files called _SQuAD_it-train.json.gz_ and _SQuAD_it-test.json.gz_, which we can decompress with the Linux `gzip` command:"],"metadata":{"id":"KJk2uTekwKWo"}},{"cell_type":"code","source":["!gzip -dkv SQuAD_it-*.json.gz"],"metadata":{"id":"H7fendyRwkV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the compressed files have been replaced with _SQuAD_it-train.json_ and _SQuAD_it-test.json_, and that the data is stored in the JSON format.\n","\n","To load a JSON file with the `load_dataset()` function, we just need to know if we're dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a `data` field. This means we can load the dataset by specifying the `field` argument as follows:"],"metadata":{"id":"UN7C4QbEwwIF"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")\n","squad_it_dataset  # Inspect object"],"metadata":{"id":"y8QCZRIVxlCR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this by inspecting the `squad_it_dataset` object above. It also shows us the number of rows and the column names associated with the training set. We can view one of the examples by indexing into the `train` split as follows:"],"metadata":{"id":"My8A1euExwEY"}},{"cell_type":"code","source":["squad_it_dataset[\"train\"][0]"],"metadata":{"id":"PefnuYYeyk4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great, we've loaded our first local dataset! But while this worked for the training set, what we really want is to include both the `train` and `test` splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the `data_files` argument that maps each split name to a file associated with that split:"],"metadata":{"id":"NO9wIobPyq-W"}},{"cell_type":"code","source":["data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n","squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n","squad_it_dataset"],"metadata":{"id":"qBvZNnLizIA5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n","\n","Hint: The `data_files` argument of the `load_dataset()` function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting `data_files=\"*.json\"`). See the [HF Datasets documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) for more details.\n","\n","The loading scripts in HF Datasets actually support automatic decompression of the input files, so we could have skipped the use of `gzip` by pointing the `data_files` argument directly to the compressed files:"],"metadata":{"id":"QJ7acePoziFe"}},{"cell_type":"code","source":["data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n","squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"],"metadata":{"id":"73yGWaPv06RU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This can be useful if you don't want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point `data_files` to the compressed files and you're good to go!"],"metadata":{"id":"dRoRspTo1AQF"}},{"cell_type":"markdown","source":["#### **Loading a remote dataset**\n","\n","If you're working as a data scientist or coder in a company, there's a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the `data_files` argument of `load_dataset()` to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point `data_files` to the _SQuAD_it-*.json.gz_ URLs as follows:"],"metadata":{"id":"d3UtjdOZ1XY_"}},{"cell_type":"code","source":["url = \"https://github.com/crux82/squad-it/raw/master/\"\n","data_files = {\n","    \"train\": url + \"SQuAD_it-train.json.gz\",\n","    \"test\": url + \"SQuAD_it-test.json.gz\",\n","}\n","squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"],"metadata":{"id":"5DCzcX-i2CAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This returns the same `DatasetDict` object obtained above, but saves us the step of manually downloading and decompressing the _SQuAD_it-*.json.gz_ files."],"metadata":{"id":"1a1uwJ5p2Neu"}},{"cell_type":"markdown","source":["### **5.2. Time to slice and dice**\n","\n","Most of the time, the data you work with won't be perfectly prepared for training models. In this section we'll explore the various features that HF Datasets provides to clean up your datasets."],"metadata":{"id":"PrsmMBEa2rXT"}},{"cell_type":"markdown","source":["#### **Slicing and dicing our data**\n","\n","Similar to Pandas, HF Datasets provides several functions to manipulate the contents of `Dataset` and `DatasetDict` objects. We already encountered the `Dataset.map()` method in chapter 3, and in this section we'll explore some of the other functions at our disposal.\n","\n","For this example we'll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that's hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient's satisfaction.\n","\n","First we need to download and extract the data, which can be done with the `wget` and `unzip` commands:"],"metadata":{"id":"fjmU_7cA28Xq"}},{"cell_type":"code","source":["!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n","!unzip drugsCom_raw.zip"],"metadata":{"id":"WCVmNFOvyMR6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can load these files by using the `csv` loading script and specifying the `delimiter` argument in the `load_dataset()` function as follows:"],"metadata":{"id":"VWGBqIUByPYR"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n","# \\t is the tab character in Python\n","drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"],"metadata":{"id":"jnMtmXlHygcM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you're working with. In HF Datasets, we can create a random sample by chaining the `Dataset.shuffle()` and `Dataset.select()` functions together:"],"metadata":{"id":"u8WfIF7By9Bi"}},{"cell_type":"code","source":["drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n","# Peek at the first few examples\n","drug_sample[:3]"],"metadata":{"id":"CqGhsveXzUlX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we've fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.select()` expects an iterable of indices, so we've passed `range(1000)` to grab the first 1,000 examples from the shuffled dataset. From this sample we can already see a few quirks in our dataset:\n","- The `Unnamed: 0` column looks suspiciously like an anonymized ID for each patient.\n","- The `condition` column includes a mix of uppercase and lowercase labels.\n","- The reviews are of varying length and contain a mix of Python line separators (`\\r\\n`) as well as HTML character codes like `&\\#039;`.\n","\n","Let's see how we can use HF Datasets to deal with each of these issues. To test the patient ID hypothesis for the `Unnamed: 0` column, we can use the `Dataset.unique()` function to verify that the number of IDs matches the number of rows in each split:"],"metadata":{"id":"_bLethoTzhl6"}},{"cell_type":"code","source":["for split in drug_dataset.keys():\n","    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"],"metadata":{"id":"NJihiQ_J02U5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This seems to confirm our hypothesis, so let's clean up the dataset a bit by renaming the `Unnamed: 0` column to something a bit more interpretable. We can use the `DatasetDict.rename_column()` function to rename the column across both splits in one go:"],"metadata":{"id":"5bKAy9g51TRn"}},{"cell_type":"code","source":["drug_dataset = drug_dataset.rename_column(\n","    original_column_name=\"Unnamed: 0\",\n","    new_column_name=\"patient_id\",\n",")\n","drug_dataset"],"metadata":{"id":"kjEr7REq1lU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, let's normalize all the `condition` labels using `Dataset.map()`. As we did with tokenization in chapter 3, we can define a simple function that can be applied across all the rows of each split in `drug_dataset`:"],"metadata":{"id":"Se2HAUXF14Mc"}},{"cell_type":"code","source":["def lowercase_condition(example):\n","    return {\"condition\": example[\"condition\"].lower()}\n","\n","\n","drug_dataset.map(lowercase_condition)"],"metadata":{"id":"TrOobU9I2GYB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oh no, we've run into a problem with our map function! From the error we can infer that some of the entries in the `condition` column are `None`, which cannot be lowercased as they're not strings. Let's drop these rows using `Dataset.filter()`, which works in a similar way to `Dataset.map()` and expects a function that receives a single example of the dataset. Instead of writing an explicit function like:"],"metadata":{"id":"I8sOboGh2bEK"}},{"cell_type":"code","source":["def filter_nones(x):\n","    return x[\"condition\"] is not None"],"metadata":{"id":"IE83dPOW23X0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and then running `drug_dataset.filter(filter_nones)`, we can do this in one line using a _lambda function_. In Python, lambda functions are small functions that you can define without explicitly naming them. They take the general form:"],"metadata":{"id":"n-pX2ERs2_5E"}},{"cell_type":"code","source":["lambda <arguments> : <expression>"],"metadata":{"id":"6udU03VV3UwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["where `lambda` is one of Python's special [keywords](https://docs.python.org/3/reference/lexical_analysis.html#keywords), `<arguments>` is a list/set of comma-separated values that define the inputs to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows:"],"metadata":{"id":"QS7UkxFt3aAz"}},{"cell_type":"code","source":["lambda x: x * x"],"metadata":{"id":"SToSTggA3vDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To apply this function to an input, we need to wrap it and the input in parantheses:"],"metadata":{"id":"tRu3jux83yb7"}},{"cell_type":"code","source":["(lambda x: x * x)(3)"],"metadata":{"id":"znpmyeZn34cL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similarly, we can define lambda functions with multiple arguments by separating them with commas. For example, we can compute the area of a triangle as follows:"],"metadata":{"id":"7-38oQTf4AvU"}},{"cell_type":"code","source":["(lambda base, height: 0.5 * base * height)(4, 8)"],"metadata":{"id":"_YNENXRV4KVA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lambda functions are handy when you want to define small, single-use functions. In the HF Datasets context, we can use lambda functions to define simple map and filter operations, so let's use this trick to eliminate the `None` entries in our dataset:"],"metadata":{"id":"oWjsrcgY4WyF"}},{"cell_type":"code","source":["drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"],"metadata":{"id":"FZSPud-L4xEu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the `None` entries removed, we can normalize our `condition` column:"],"metadata":{"id":"J0MYzQ1E48Mj"}},{"cell_type":"code","source":["drug_dataset = drug_dataset.map(lowercase_condition)\n","# Check that lowercasing worked\n","drug_dataset[\"train\"][\"condition\"][:3]"],"metadata":{"id":"G49PZB595Cd_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Creating new columns**\n","\n","Whenever you're dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like \"Great!\" or a full-blown essay with thousands of words, and depending on the use case you'll need to handle these extremes differently. To compute the number of words in each review, we'll use a rough heuristic based on splitting each text by whitespace.\n","\n","Let's define a simple function that counts the number of words in each review:"],"metadata":{"id":"4enjgwJx5bmI"}},{"cell_type":"code","source":["def compute_review_length(example):\n","    return {\"review_length\": len(example[\"review\"].split())}"],"metadata":{"id":"vczUVybX6Y-7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unlike our `lowercase_condition()` function, `compute_review_length()` returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when `compute_review_length()` is passed to `Dataset.map()`, it will be applied to all the rows in the dataset to create a new `review_length` column:"],"metadata":{"id":"R8HJJnot6nmP"}},{"cell_type":"code","source":["drug_dataset = drug_dataset.map(compute_review_length)\n","# Inspect the first training example\n","drug_dataset[\"train\"][0]"],"metadata":{"id":"4EpeQwyZ7E-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As expected, we can see a `review_length` column has been added to our training set.\n","\n","Hint: An alternative way to add new columns to a dataset is with the `Dataset.add_column()` function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where `Dataset.map()` is not well suited for your analysis.\n","\n","We can sort the new column with `Dataset.sort()` to see what the extreme values look like:"],"metadata":{"id":"8GTkz7wQ7RrE"}},{"cell_type":"code","source":["drug_dataset[\"train\"].sort(\"review_length\")[:3]"],"metadata":{"id":"leJLEQD37c4F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we suspected, some reviews contain just a single word, which, although it may be okay for sentiment analysis, would not be informative if we want to predict the condition.\n","\n","Let's use the `Dataset.filter()` function to remove reviews that contain fewer than 30 words. Similarly to what we did with the `condition` column, we can filter out the very short reviews by requiring that the reviews have a length above this threshold:"],"metadata":{"id":"7o9F-FK07oTT"}},{"cell_type":"code","source":["drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n","print(drug_dataset.num_rows)"],"metadata":{"id":"tE5jwhaL8kCV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, this has removed around 15% of the reviews from our original training and test sets.\n","\n","The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python's `html` module to unescape these characters, like so:"],"metadata":{"id":"AA3yhc078xQQ"}},{"cell_type":"code","source":["import html\n","\n","text = \"I&#039;m a transformer called BERT\"\n","html.unescape(text)"],"metadata":{"id":"GyIGJibs9IGf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll use `Dataset.map()` to unescape all the HTML characters in our corpus:"],"metadata":{"id":"N85ReDCm9PCl"}},{"cell_type":"code","source":["drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"],"metadata":{"id":"cuFMhHhv9Vbd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **The `map()` method's superpowers**\n","\n","The `Dataset.map()` method takes a `batched` argument that, if set to `True`, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000). For instance, the previous map function that unescaped all the HTML took a bit of time to run. We can speed this up by processing several elements at the same time using a list comprehension.\n","\n","When you specify `batched=True` the function receives a dictionary with the fields of the dataset, but each value is now a _list of values_, and not just a single value. The return value of `Dataset.map()` should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. For example, here is another way to unescape all HTML characters, but using `batched=True`:"],"metadata":{"id":"WitzqSZy9yWb"}},{"cell_type":"code","source":["new_drug_dataset = drug_dataset.map(\n","    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",")"],"metadata":{"id":"ef36Afe7TE5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You'll see that this command executes way faster than the previous one. This is because list comprehensions are usually faster than executing the same code in a `for` loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.\n","\n","Using `Dataset.map()` with `batched=True` will be essential to unlock the speed of the \"fast\" tokenizers that we'll encounter in chapter 6, which can quickly tokenize big lists of texts. For instance, to tokenize all the drug reviews with a fast tokenizer, we could use a function like this:"],"metadata":{"id":"dZp4iTJcTZOg"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"review\"], truncation=True)"],"metadata":{"id":"lV0gWN2DVFEO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can pass one or several examples to the tokenizer, so we can use this function with or without `batched=True`. Let's take this opportunity to compare the performance of the different options. In a notebook, you can time a one-line instruction by adding `%time` before the line of code you wish to measure:"],"metadata":{"id":"3bawhkO6VUXk"}},{"cell_type":"code","source":["%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"],"metadata":{"id":"8BI8BngRWSAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also time a whole cell by putting `%%time` at the beginning of the cell.\n","\n","In this case, using a fast tokenizer with the `batched=True` option can be 30 times faster than its slow counterpart with no batching - this is truly amazing! Fast tokenizers are able to achieve such a speedup because behind the scenes the tokenization code is executed in Rust, which is a language that makes it easy to parallelize code execution.\n","\n","Parallelization is also the reason for the nearly 6x speedup the fast tokenizer achieves with batching: you can't parallelize a single tokenization operation, but when you want to tokenize lots of texts at the same time you can just split the execution across several processes, each responsible for its own texts.\n","\n","`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by Rust, they won't let a slow tokenizer catch up with a fast one, but they can still be helpful (especially if you're using a tokenizer that doesn't have a fast version). To enable multiprocessing, use the `num_proc` argument and specify the number of processes to use in your call to `Dataset.map()`:"],"metadata":{"id":"isLZQ34DWfzG"}},{"cell_type":"code","source":["slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n","\n","\n","def slow_tokenize_function(examples):\n","    return slow_tokenizer(examples[\"review\"], truncation=True)\n","\n","\n","tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"],"metadata":{"id":"YI2s4mfkYWGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can experiment a little with timing to determine the optimal number of processes to use; in our case 8 seemed to produce the best speed gain.\n","\n","The performance of the fast tokenizer can also improve substantially. Note, however, that won't always be the case - in this example, for values of `num_proc` other than 8, our tests showed that it was faster to use `batched=True` without that option. In general, we don't recommend using Python multiprocessing for fast tokenizers with `batched=True`.\n","\n","Hint: Using `num_proc` to speed up your processing is usually a great idea, as long as the function you are using is not already doing some kind of multiprocessing of its own.\n","\n","With `Dataset.map()` and `batched=True` you can also change the number of elements in your dataset. This is super useful in many situations where you want to create several training features from one example.\n","\n","Hint: In machine learning, an _example_ is usually defined as the set of _features_ that we feed to the model. In some contexts, these features will be the set of columns in a `Dataset`, but in others (like here and for question answering), multiple features can be extracted from a single example and belong to a single column.\n","\n","Let's have a look at how it works! Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return _all_ the chunks of the texts instead of just the first one. This can be done with `return_overflowing_tokens=True`:"],"metadata":{"id":"EbeauBHxYsGI"}},{"cell_type":"code","source":["def tokenize_and_split(examples):\n","    return tokenizer(\n","        examples[\"review\"],\n","        truncation=True,\n","        max_length=128,\n","        return_overflowing_tokens=True,\n","    )\n","\n","\n","# Test function on one example before using `Dataset.map()` on the whole dataset\n","result = tokenize_and_split(drug_dataset[\"train\"][0])\n","[len(inp) for inp in result[\"input_ids\"]]"],"metadata":{"id":"HxdlQJKJcNLe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So, our first example in the training set became two features because it was tokenized to more than the maximum number of tokens we specified: the first one of length 128 and the second one of length 49. Now let's do this for all elements of the dataset!"],"metadata":{"id":"BVKYkUfAccvW"}},{"cell_type":"code","source":["tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n","# Throws: ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000"],"metadata":{"id":"rPY2Zq6PdTn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oh no! That didn't work! Why not? Looking at the error message will give us a clue: there is a mismatch in the length of one of the columns, one being of length 1,463 and the other of length 1,000. If you've looked at the `Dataset.map()` documentation, you may recall that it's the number of samples passed to the function that we are mapping; here those 1,000 examples gave 1,463 new features, resulting in a shape error.\n","\n","The problem is that we're trying to mix two different datasets of different sizes: the `drug_dataset` columns will have a certain number of examples (the 1,000 in our error), but the `tokenized_dataset` we are building will have more (it is more than 1,000 because we are tokenizing long reviews into more than one example by using `return_overflowing_tokens=True`). That doesn't work for a `Dataset`, so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset. We can do the former with the `remove_columns` argument:"],"metadata":{"id":"VZbRAAJkdi3c"}},{"cell_type":"code","source":["tokenized_dataset = drug_dataset.map(\n","    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names,\n",")"],"metadata":{"id":"vyjwjt08eprc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now this works without error. We can check that our new dataset has many more elements than the original dataset by comparing the lengths:"],"metadata":{"id":"_rAptgiQe2Va"}},{"cell_type":"code","source":["len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"],"metadata":{"id":"pqJBmLJzfAqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We mentioned that we can also deal with the mismatched length problem by making the old columns the same size as the new ones. To do this, we will need the `overflow_to_sample_mapping` field the tokenizer returns when we set `return_overflowing_tokens=True`. It gives us a mapping from a new feature index to the index of the sample it originated from. Using this, we can associate each key present in our original dataset with a list of values of the right size by repeating the values of each example as many times as it generates new features:"],"metadata":{"id":"v6qSUoXlfJVp"}},{"cell_type":"code","source":["def tokenize_and_split(examples):\n","    result = tokenizer(\n","        examples[\"review\"],\n","        truncation=True,\n","        max_length=128,\n","        return_overflowing_tokens=True,\n","    )\n","    # Extract mapping between new and old indices\n","    sample_map = result.pop(\"overflow_to_sample_mapping\")\n","    for key, values in examples.items():\n","        result[key] = [values[i] for i in sample_map]\n","    return result"],"metadata":{"id":"nK1MKkz3fv1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see it works with `Dataset.map()` without us needing to remove the old columns:"],"metadata":{"id":"-7LpZJ0OgRle"}},{"cell_type":"code","source":["tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n","tokenized_dataset"],"metadata":{"id":"0rvixg9NgXjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get the same number of training features as before, but here we've kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach.\n","\n","You've now seen how HF Datasets can be used to preprocess a dataset in various ways. Although the processing functions of HF Datasets will cover most of your model training needs, there may be times when you'll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, HF Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX."],"metadata":{"id":"9Ejgi3zhgd_H"}},{"cell_type":"markdown","source":["#### **From `Dataset`s to `DataFrame`s and back**\n","\n","To enable the conversion between various third-party libraries, HF Datasets provides a `Dataset.set_format()` function. This function only changes the _output format_ of the dataset, so you can easily switch to another format without affecting the underlying _data format_, which is Apache Arrow. The formatting is done in place. To demonstrate, let's convert our dataset to Pandas:"],"metadata":{"id":"wyVMfiBrrusQ"}},{"cell_type":"code","source":["drug_dataset.set_format(\"pandas\")"],"metadata":{"id":"eEeZjK2H8MYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now when we access elements of the dataset we get a `pandas.DataFrame` instead of a dictionary:"],"metadata":{"id":"lttc-Epp8Qgk"}},{"cell_type":"code","source":["drug_dataset[\"train\"][:3]"],"metadata":{"id":"2ewBqnDV8YFT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_dataset[\"train\"]`:"],"metadata":{"id":"rM4MNkzm8ezb"}},{"cell_type":"code","source":["train_df = drug_dataset[\"train\"][:]"],"metadata":{"id":"uRk4Abjh8pXQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hint: Under the hood, `Dataset.set_format()` changes the return format for the dataset's `__getitem__()` dunder/magic method. This means that when we want to create a new object like `train_df` from a `Dataset` in the `\"pandas\"` format, we need to slice the whole dataset to obtain a `pandas.DataFrame`. You can verify for yourself that the type of `drug_dataset[\"train\"]` is `Dataset`, irrespective of the output format.\n","\n","From here we can use all the Pandas functionality that we want. For example we can do fancy chaining to compute the class distribution among the `condition` entries:"],"metadata":{"id":"4d6fE9EP8u2H"}},{"cell_type":"code","source":["frequencies = (\n","    train_df[\"condition\"]\n","    .value_counts()\n","    .to_frame()\n","    .reset_index()\n","    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",")\n","frequencies.head()"],"metadata":{"id":"JAHSvf_9917i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And once we're done with our Pandas analysis, we can always create a new `Dataset` object by using the `Dataset.from_pandas()` function as follows:"],"metadata":{"id":"ls2neOp3-T-b"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","freq_dataset = Dataset.from_pandas(frequencies)\n","freq_dataset"],"metadata":{"id":"rJI8TFy7-dT6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This wraps up our tour of the various preprocessing techniques available in HF Datasets. To round out the section, let's create a validation set to prepare the dataset for training a classifier on. Before doing so, we'll reset the output format of `drug_dataset` from `\"pandas\"` to `\"arrow\"` (otherwise you can run into problems when you try to tokenize your text because it is no longer represented as strings in a dictionary):"],"metadata":{"id":"Ici0-0OY-q-X"}},{"cell_type":"code","source":["drug_dataset.reset_format()"],"metadata":{"id":"Jqtyr-J0_EA5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Creating a validation set**\n","\n","Although we have a test set we could use for evaluation, it's a good practice to leave the test set untouched and create a separate validation set during development. Once you are happy with the performance of your models on the validation set, you can do a final sanity check on the test set. This process helps mitigate the risk that you'll overfit to the test set and deploy a model that fails on real-world data.\n","\n","HF Datasets provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn`. Let's use it to split our training set into `train` and `validation` splits (we set the `seed` argument for reproducibility):"],"metadata":{"id":"FeBM1WOq_nrc"}},{"cell_type":"code","source":["drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n","# Rename the default \"test\" split to \"validation\"\n","drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n","# Add the \"test\" set to our `DatasetDict`\n","drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n","drug_dataset_clean"],"metadata":{"id":"Eh3ttcrqAcEO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great, we've now prepared a dataset that's ready for training some models on! In section 5 we'll show you how to upload datasets to the Hugging Face Hub, but for now let's cap off our analysis by looking at a few ways you can save datasets on your local machine."],"metadata":{"id":"AE_JgX99BPCY"}},{"cell_type":"markdown","source":["#### **Saving a dataset**\n","\n","Although HF Datasets will cache every downloaded dataset and the operations performed on it, there are times when you'll want to save a dataset to disk (e.g., in case the cache gets deleted). As shown in the table below, HF Datasets provides four main functions to save your dataset in different formats:\n","\n","| Data format | Function |\n","| --- | --- |\n","| Arrow | `Dataset.save_to_disk()` |\n","| CSV | `Dataset.to_csv()` |\n","| JSON | `Dataset.to_json()` |\n","| Parquet | `Dataset.to_parquet()` |\n","\n","Hint: The CSV and JSON formats are great if you want to save small to medium-sized datasets. But if your dataset is huge, you'll want to save it in either the Arrow or Parquet formats. Arrow files are great if you plan to reload or process the data in the near future. Parquet files are designed for long-term disk storage and are very space efficient.\n","\n","For example, let's save our cleaned dataset in the Arrow format:"],"metadata":{"id":"pdV9b_XnBg6g"}},{"cell_type":"code","source":["drug_dataset_clean.save_to_disk(\"drug-reviews\")"],"metadata":{"id":"K1G8PJgfGE0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will create a directory with the following structure:\n","```\n","drug-reviews/\n","|-- dataset_dict.json\n","|-- test\n","|   |-- dataset.arrow\n","|   |-- dataset_info.json\n","|   '-- state.json\n","|-- train\n","|   |-- dataset.arrow\n","|   |-- dataset_info.json\n","|   |-- indices.arrow\n","|   '-- state.json\n","'-- validation\n","    |-- dataset.arrow\n","    |-- dataset_info.json\n","    |-- indices.arrow\n","    '-- state.json\n","```\n","where we can see that each split is associated with its own _dataset.arrow_ table, and some metadata in _dataset_info.json_ and _state.json_. You can think of the Arrow format as a fancy table of columns and rows that is optimized for building high-performance applications that process and transport large datasets.\n","\n","Once the dataset is saved, we can load it by using the `load_from_disk()` function as follows:"],"metadata":{"id":"MZsGtANYGIRP"}},{"cell_type":"code","source":["from datasets import load_from_disk\n","\n","drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n","drug_dataset_reloaded"],"metadata":{"id":"oQD1LtKhH6Cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the CSV, JSON and Parquet formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object:"],"metadata":{"id":"LJLqI8P7IBi2"}},{"cell_type":"code","source":["for split, dataset in drug_dataset_clean.items():\n","    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"],"metadata":{"id":"2L9TtEcoIPSd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON. Here's what the first example looks like:"],"metadata":{"id":"K6vL0CQhIZwb"}},{"cell_type":"code","source":["!head -n 1 drug-reviews-train.jsonl"],"metadata":{"id":"_h-SE2WXIiyZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can then use the techniques from section 2 to load the JSON files as follows:"],"metadata":{"id":"PoWCtgPXInHV"}},{"cell_type":"code","source":["data_files = {\n","    \"train\": \"drug-reviews-train.jsonl\",\n","    \"validation\": \"drug-reviews-validation.jsonl\",\n","    \"test\": \"drug-reviews-test.jsonl\",\n","}\n","drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"],"metadata":{"id":"q1LHw8imIthG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And that's it for our excursion into data wrangling with HF Datasets! Now that we have a cleaned dataset for training a model on, here are a few ideas that you could try out:\n","1. Use the techniques from chapter 3 to train a classifier that can predict the patient condition based on the drug review.\n","2. Use the `summarization` pipeline from chapter 1 to generate summaries of the reviews.\n","\n","Next, we'll take a look at how HF Datasets can enable you to work with huge datasets without blowing up your laptop!"],"metadata":{"id":"Fg578vBEJAwg"}},{"cell_type":"markdown","source":["### **5.3. Big data? HF Datasets to the rescue!**\n","\n","Nowadays it is not uncommon to find yourself working with multi-gigabyte datasets, especially if you're planning to pretrain a transformer like BERT or GPT-2 from scratch. In these cases, even _loading_ the data can be a challenge. For example, the WebText corpus used to pretrain GPT-2 consists of over 8 million documents and 40 GB of text - loading this into your laptop's RAM is likely to give it a heart attack!\n","\n","Fortunately, HF Datasets has been designed to overcome these limitations. It frees you from memory management problems by treating datasets as _memory-mapped_ files (Apache Arrow format), and from hard drive limits by _streaming_ the entries in a corpus (`IterableDataset`).\n","\n","Memory mapping is a mechanism that maps a portion of a file or an entire file on disk to a chunk of virtual memory. This allows applications to access segments of an extremely large file without having to read the entire file into memory first. The Apache Arrow format is designed for high-performance data processing and represents each table-like dataset with an in-memory columnar format. Columnar formats group the elements of a table in consecutive blocks of RAM and this unlocks fast access and processing. Another cool feature of Arrow's memory mapping capability is that it allows multiple processes to work with the same large dataset without moving it or copying it in any way. This \"zero-copy\" feature of Arrow makes it extremely fast for iterating over a dataset.\n","\n","Arrow is great at processing data at any scale, but some datasets are so large that you can't even fit them on your hard disk. For these cases, the HF Datasets library provides a streaming API that allows you to progressively download the raw data one element at a time. To stream a large dataset, the only change you need to make is to set the `streaming=True` argument in the `load_dataset()` function. This will return a special `IterableDataset` object, which is a bit different to the `Dataset` object. This object is an iterable, which means we can't index it to access elements, but instead iterate on it using the `iter()` and `next()` methods. This will download and access a single example from the dataset, which means you can progressively iterate through a huge dataset without having to download it first. The main difference with an `IterableDataset` is that instead of using the `select()` method to return examples, we use the `take()` and `skip()` methods because we can't index into the dataset. The `take()` method returns the first N examples in the dataset, while `skip()` skips the first N and returns the rest.\n","\n","In this section we'll explore these features of HF Datasets with a huge 825 GB corpus known as [the Pile](https://pile.eleuther.ai/)."],"metadata":{"id":"Gr_WtISUKmfN"}},{"cell_type":"markdown","source":["#### **What is the Pile?**\n","\n","The Pile is an English text corpus that was created by [EleutherAI](https://www.eleuther.ai/) for training large-scale language models. It includes a diverse range of datasets, spanning scientific articles, GitHub code repositories, and filtered web text. The training corpus is available in [14 GB chunks](https://the-eye.eu/public/AI/pile/), and you can also download several of the [individual components](https://the-eye.eu/public/AI/pile_preliminary_components/). Let's start by taking a look at the PubMed Abstracts dataset, which is a corpus of abstracts from 15 million biomedical publications on [PubMed](https://pubmed.ncbi.nlm.nih.gov/). The dataset is in JSON Lines format and is compressed using the `zstandard` library, so first we need to install that:"],"metadata":{"id":"hKuqm5Km5nvw"}},{"cell_type":"code","source":["!pip install zstandard"],"metadata":{"id":"UvKmEbLx6zir"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we can load the dataset using the method for remote files that we learned in section 2:"],"metadata":{"id":"ANbiqJ1162-7"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# This takes a few minutes to run\n","data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n","pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n","pubmed_dataset"],"metadata":{"id":"RwMv5HBY6_Bz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hint: By default, HF Datasets will decompress the files needed to load a dataset. If you want to preserve hard drive space, you can pass `DownloadConfig(delete_extracted=True)` to the `download_config` argument of `load_dataset()`. See the [documentation](https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig) for more details.\n","\n","Now let's see how much RAM we've used to load the dataset!"],"metadata":{"id":"PKpyflkP7kbu"}},{"cell_type":"markdown","source":["#### **The magic of memory mapping**\n","\n","A simple way to measure memory usage in Python is with the [`psutil`](https://psutil.readthedocs.io/en/latest/) library, which can be installed with `pip` as follows:"],"metadata":{"id":"neSoIRZ3-V7c"}},{"cell_type":"code","source":["!pip install psutil"],"metadata":{"id":"c59DwdHP-jdJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It provides a `Process` class that allows us to check the memory usage of the current process as follows:"],"metadata":{"id":"_LQS9zDA-oiv"}},{"cell_type":"code","source":["import psutil\n","\n","# Process.memory_info is expressed in bytes, so convert to megabytes\n","print(f\"RAM used : {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"],"metadata":{"id":"V9n_ebIH-3Rm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here the `rss` attribute refers to the _resident set size_, which is the fraction of memory that a process occupies in RAM. This measurement also includes the memory used by the Python interpreter and the libraries we've loaded, so the actual amount of memory used to load the dataset is a bit smaller. For comparison, let's see how large the dataset is on disk, using the `dataset_size` attribute. Since the result is expressed in bytes like before, we need to manually convert it to gigabytes:"],"metadata":{"id":"eleYjLUK_cwF"}},{"cell_type":"code","source":["print(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\n","size_gb = pubmed_dataset.dataset_size / (1024**3)\n","print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"],"metadata":{"id":"82VU2_D5_8nN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice - despite it being almost 20 GB large, we're able to load and access the dataset with much less RAM!\n","\n","If you're familiar with Pandas, this result might come as a surprise because of Wes Kinney's famous [rule of thumb](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) that you typically need 5 to 10 times as much RAM as the size of your dataset. So how does HF Datasets solve this memory management problem? HF Datasets treats each dataset as a [memory-mapped file](https://en.wikipedia.org/wiki/Memory-mapped_file), which provides a mapping between RAM and filesystem storage that allows the library to access and operate on elements of the dataset without needing to fully load it into memory.\n","\n","Memory-mapped files can also be shared across multiple processes, which enables methods like `Dataset.map()` to be parallelized without needing to move or copy the dataset. Under the hood, these capabilities are all realized by the [Apache Arrow](https://arrow.apache.org/) memory format and [`pyarrow`](https://arrow.apache.org/docs/python/index.html) library, which make the data loading and processing lightning fast. To see this in action, let's run a little speed test by iterating over all the elements in the PubMed Abstracts dataset:"],"metadata":{"id":"LRWdDj6hAtSx"}},{"cell_type":"code","source":["import timeit\n","\n","code_snippet = \"\"\"batch_size = 1000\n","\n","for idx in range(0, len(pubmed_dataset), batch_size):\n","    _ = pubmed_dataset[idx:idx + batch_size]\n","\"\"\"\n","\n","time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n","print(\n","    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n","    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",")"],"metadata":{"id":"99d-NNGkCc1e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we've used Python's `timeit` module to measure the execution time taken by `code_snippet`. You'll typically be able to iterate over a dataset at speed of a few tenths of a GB/s to several GB/s. This works great for the vast majority of applications, but sometimes you'll have to work with a dataset that is too large to even store on your laptop's hard drive. For example, if we tried to download the Pile in its entirety, we'd need 825 GB of free disk space! To handle these cases, HF Datasets provides a streaming feature that allows us to download and access elements on the fly, without needing to download the whole dataset.\n","\n","Hint: In Jupyter notebooks you can also time cells using the [`%%timeit` magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)."],"metadata":{"id":"Oo3_bD5PDjFv"}},{"cell_type":"markdown","source":["#### **Streaming datasets**\n","\n","To enable dataset streaming you just need to pass the `streaming=True` argument to the `load_dataset()` function. For example, let's load the PubMed Abstracts dataset again, but in streaming mode:"],"metadata":{"id":"LPHvDtnOAtRH"}},{"cell_type":"code","source":["pubmed_dataset_streamed = load_dataset(\n","    \"json\", data_files=data_files, split=\"train\", streaming=True,\n",")"],"metadata":{"id":"_TmEeXFXFCPW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of the familiar `Dataset` that we've encountered elsewhere in this chapter, the object returned with `streaming=True` is an `IterableDataset`. As the name suggests, to access the elements of an `IterableDataset` we need to iterate over it. We can access the first element of our streamed dataset as follows:"],"metadata":{"id":"eDTy40wzFb3z"}},{"cell_type":"code","source":["next(iter(pubmed_dataset_streamed))"],"metadata":{"id":"_vcBCfu5GADt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The elements from a streamed dataset can be processed on the fly using `IterableDataset.map()`, which is useful during training if you need to tokenize the inputs. The process is exactly the same as the one we used to tokenize our dataset in chapter 3, with the only difference being that outputs are returned one by one:"],"metadata":{"id":"OJItV8n9GDWT"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n","next(iter(tokenized_dataset))"],"metadata":{"id":"InLGJja2Hcbz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hint: To speed up tokenization with streaming you can pass `batched=True`, as we saw in the last section. It will process the examples batch by batch; the default batch size is 1,000 and can be specified with the `batch_size` argument.\n","\n","You can also shuffle a streamed dataset using `IterableDataset.shuffle()`, but unlike `Dataset.shuffle()` this only shuffles the elements in a predefined `buffer_size`:"],"metadata":{"id":"Pc672u97H6OP"}},{"cell_type":"code","source":["shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n","next(iter(shuffled_dataset))"],"metadata":{"id":"Jd3TlWBLIYUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this example, we selected a random example from the first 10,000 examples in the buffer. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the 10,001st example in the case above). You can also select elements from a streamed dataset using the `IterableDataset.take()` and `IterableDataset.skip()` functions, which act in a similar way to `Dataset.select()`. For example, to select the first 5 examples in the PubMed Abstracts dataset we can do the following:"],"metadata":{"id":"pDR8pzkTIuNp"}},{"cell_type":"code","source":["dataset_head = pubmed_dataset_streamed.take(5)\n","list(dataset_head)"],"metadata":{"id":"w-MYm9dNJZBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similarly, you can use the `IterableDataset.skip()` function to create training and validation splits from a shuffled dataset as follows:"],"metadata":{"id":"usV75qA9JgvR"}},{"cell_type":"code","source":["# Skip the first 1,000 examples and include the rest in the training set\n","train_dataset = shuffled_dataset.skip(1000)\n","# Take the first 1,000 examples for the validation set\n","validation_dataset = shuffled_dataset.take(1000)"],"metadata":{"id":"cxbKCCMmJrKv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's round out our exploration of dataset streaming with a common application: combining multiple datasets together to create a single corpus. HF Datasets provides an `interleave_datasets()` function that converts a list of `IterableDataset` objects into a single `IterableDataset`, where the elements of the new dataset are obtained by alternating among the source examples. This function is especially useful when you're trying to combine large datasets, so as an example let's stream the FreeLaw subset of the Pile, which is a 51 GB dataset of legal opinions from US courts:"],"metadata":{"id":"JqgyBRXIJ7O5"}},{"cell_type":"code","source":["law_dataset_streamed = load_dataset(\n","    \"json\",\n","    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n","    split=\"train\",\n","    streaming=True,\n",")\n","next(iter(law_dataset_streamed))"],"metadata":{"id":"tF6tDrTlKq-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This dataset is large enough to stress the RAM of most laptops, yet we've been able to load and access it without breaking a sweat! Let's now combine the examples from the FreeLaw and PubMed Abstracts datasets with the `interleave_datasets()` function:"],"metadata":{"id":"11uzp9PnK0dL"}},{"cell_type":"code","source":["from itertools import islice\n","from datasets import interleave_datasets\n","\n","combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\n","list(islice(combined_dataset, 2))"],"metadata":{"id":"k0DQWKsgLLCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we've used the `islice()` function from Python's `itertools` module to select the first two examples from the combined dataset, and we can see that they match the first examples from each of the two source datasets.\n","\n","Finally, if you want to stream the Pile in its 825 GB entirety, you can grab all the prepared files as follows:"],"metadata":{"id":"a8s9OKLPLiHE"}},{"cell_type":"code","source":["base_url = \"https://the-eye.eu/public/AI/pile/\"\n","data_files = {\n","    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n","    \"validation\": base_url + \"val.jsonl.zst\",\n","    \"test\": base_url + \"test.jsonl.zst\",\n","}\n","pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n","next(iter(pile_dataset[\"train\"]))"],"metadata":{"id":"4_WLYxcVPRAz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You now have all the tools you need to load and process datasets of all shapes and sizes - but unless you're exceptionally lucky, there will come a point in your NLP journey where you'll have to actually create a dataset to solve the problem at hand. That's the topic of the next section!"],"metadata":{"id":"9sOhmUrRK0ba"}},{"cell_type":"markdown","source":["### **5.4. Creating your own dataset**\n","\n","Sometimes the dataset that you need to build an NLP application doesn't exist, so you'll need to create it yourself. In this section we'll show you how to create a corpus of GitHub issues, which are commonly used to track bugs or features in GitHub repositories. This corpus could be used for various purposes, including:\n","\n","* Exploring how long it takes to close open issues or pull requests\n","* Training a _multilabel classifier_ that can tag issues with metadata based on the issue's description (e.g., \"bug\", \"enhancement\", or \"question\")\n","* Creating a semantic search engine to find which issues match a user's query\n","\n","Here we'll focus on creating the corpus, and in the next section we'll tackle the semantic search application. To keep things meta, we'll use the GitHub issues associated with a popular open source project: HF Datasets! Let's take a look at how to get the data and explore the information contained in these issues."],"metadata":{"id":"v3omGQG536pf"}},{"cell_type":"markdown","source":["#### **Getting the data**\n","\n","You can find all the issues in HF Datasets by navigating to the repository's [Issues tab](https://github.com/huggingface/datasets/issues). If you click on one of these issues you'll find it contains a title, a description, and a set of labels that characterize the issue. To download all the repository's issues, we'll use the [GitHub REST API](https://docs.github.com/en/rest) to poll the [`Issues` endpoint](https://docs.github.com/en/rest/reference/issues#list-repository-issues). This endpoint returns a list of JSON objects, with each object containing a large number of fields that include the title and description as well as metadata about the status of the issue and so on.\n","\n","A convenient way to download the issues is via the `requests` library, which is the standard way for making HTTP requests in Python. You can install the library by running:"],"metadata":{"id":"U1_88KW4fvIS"}},{"cell_type":"code","source":["!pip install requests"],"metadata":{"id":"tkSWKh9Ag9qy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the library is installed, you can make GET requests to the `Issues` endpoint by invoking the `requests.get()` function. The _payload_ can then be accessed in varous formats like bytes, strings, or JSON.\n","\n","As described in the GitHub [documentation](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting), unauthenticated requests are limited to 60 requests per hour. Although you can increase the `per_page` query parameter to reduce the number of requests you make, you still hit the rate limit on any repository that has more than a few thousand issues. So instead, you should follow GitHub's [instructions](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) on creating a _personal access token_ so that you can boost the rate limit to 5,000 requests per hour. Once you have your token, you can include it as a part of the request header.\n","\n","Let's create a function that can download all the issues from a GitHub repository:"],"metadata":{"id":"J0SrylD2g_2A"}},{"cell_type":"code","source":["import time\n","import math\n","from pathlib import Path\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","\n","GITHUB_TOKEN = \"<TOKEN>\"  # Put in your GitHub token here\n","headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n","\n","\n","def fetch_issues(\n","    owner=\"huggingface\",\n","    repo=\"datasets\",\n","    num_issues=10_000,\n","    rate_limit=5_000,\n","    issues_path=Path(\".\"),\n","):\n","    if not issues_path.is_dir():\n","        issues_path.mkdir(exist_ok=True)\n","\n","    batch = []\n","    all_issues = []\n","    per_page = 100  # Number of issues to return per page\n","    num_pages = math.ceil(num_issues / per_page)\n","    base_url = \"https://api.github.com/repos\"\n","\n","    for page in tqdm(range(num_pages)):\n","        # Query with state=all to get both open and closed issues\n","        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n","        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n","        batch.extend(issues.json())\n","\n","        if len(batch) > rate_limit and len(all_issues) < num_issues:\n","            all_issues.extend(batch)\n","            batch = []  # Flush batch for next time period\n","            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n","            time.sleep(60 * 60 + 1)\n","\n","    all_issues.extend(batch)\n","    df = pd.DataFrame.from_records(all_issues)\n","    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n","    print(\n","        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n","    )\n","\n","\n","# Depending on your internet connection, this can take several minutes to run...\n","fetch_issues()"],"metadata":{"id":"pR0luf6TjEc_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now when we call `fetch_issues()` it will download all the issues in batches to avoid exceeding GitHub's limit on the number of requests per hour; the result will be stored in a _repository_name-issues.jsonl_ file, where each line is a JSON object that represents an issue.\n","\n","Once the issues are downloaded we can load them locally using our newfound skills from section 2:"],"metadata":{"id":"7Y13ztGmk-GC"}},{"cell_type":"code","source":["issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n","issues_dataset"],"metadata":{"id":"afS1QgMCl6Y2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great, we've created our first dataset from scratch! But why are there several thousand issues when the [Issues tab](https://github.com/huggingface/datasets/issues) of the HF Datasets repository only shows around 1,000 issues in total? As described in the GitHub [documentation](https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user), that's because we've downloaded all the pull requests as well. Since the contents of issues and pull requests are quite different, let's do some minor preprocessing to enable us to distinguish between them."],"metadata":{"id":"5VvS4BTml--6"}},{"cell_type":"markdown","source":["#### **Cleaning up the data**\n","\n","GitHub's documentation tells us that the `pull_request` column can be used to differentiate between issues and pull requests. Let's look at a random sample to see what the difference is. As we did in section 3, we'll chain `Dataset.shuffle()` and `Dataset.select()` to create a random sample and then zip the `html_url` and `pull_request` columns so we can compare the various URLs:"],"metadata":{"id":"n9jPYZbKmn_f"}},{"cell_type":"code","source":["sample = issues_dataset.shuffle(seed=666).select(range(3))\n","\n","# Print out the URL and pull request entries\n","for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n","    print(f\">> URL: {url}\")\n","    print(f\">> Pull request: {pr}\\n\")"],"metadata":{"id":"gMvRxkhknJgk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we can see that each pull request is associated with various URLs, while ordinary issues have a `None` entry. We can use this distinction to create a new `is_pull_request` column that checks whether the `pull_request` field is `None` or not:"],"metadata":{"id":"smEogpJnnjPe"}},{"cell_type":"code","source":["issues_dataset = issues_dataset.map(\n","    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",")"],"metadata":{"id":"KRgkHvc5n2Nv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Although we could proceed to further clean up the dataset by dropping or renaming some columns, it is generally a good practice to keep the dataset as \"raw\" as possible at this stage so that it can be easily used in multiple applications.\n","\n","Before we push our dataset to the Hugging Face Hub, let's deal with one thing that's missing from it: the comments associated with each issue and pull request. We'll add them next with - you guessed it - the GitHub REST API!"],"metadata":{"id":"BIlF1ixwoS2s"}},{"cell_type":"markdown","source":["#### **Augmenting the dataset**\n","\n","The comments associated with an issue or pull request provide a rich source of information, especially if we're interested in building a search engine to answer user queries about the library. The GitHub REST API provides a [`Comments` endpoint](https://docs.github.com/en/rest/reference/issues#list-issue-comments) that returns all the comments associated with an issue number. Comments are stored in the `body` field, so let's write a simple function that returns all the comments associated with an issue by picking out the `body` contents for each element in `response.json()`:"],"metadata":{"id":"KfPsFlaRo4A7"}},{"cell_type":"code","source":["def get_comments(issue_number):\n","    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n","    response = requests.get(url, headers=headers)\n","    return [r[\"body\"] for r in response.json()]\n","\n","\n","# Test our function works as expected\n","get_comments(2792)"],"metadata":{"id":"KerMDYWau_of"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks good, so let's use `Dataset.map()` to add a new `comments` column to each issue in our dataset:"],"metadata":{"id":"cEPkSXzlvIIt"}},{"cell_type":"code","source":["# Depending on your internet connection, this can take a few minutes...\n","issues_with_comments_dataset = issues_dataset.map(\n","    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",")"],"metadata":{"id":"CPUJlOJavQNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The final step is to push our dataset to the Hub. Let's take a look at how we can do that."],"metadata":{"id":"S9aMVyNvvXA7"}},{"cell_type":"markdown","source":["#### **Uploading the dataset to the Hugging Face Hub**\n","\n","Now that we have our augmented dataset, it's time to push it to the Hub so we can share it with the community! Uploading a dataset is very simple: just like models and tokenizers from HF Transformers, we can use a `push_to_hub()` method to push a dataset. To do that we need an authentication token, which can be obtained by first logging into the Hugging Face Hub with the `notebook_login()` function:"],"metadata":{"id":"JGYhA0tEvfec"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"BUfRld2aZDBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will create a widget where you can enter your username and password, and an API token will be saved in _~/.huggingface/token_. If you're running the code in a terminal, you can log in via the CLI instead:"],"metadata":{"id":"sxm2rhEVZE_G"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"u03R9369ZRCQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we've done this, we can upload our dataset by running:"],"metadata":{"id":"ovyPxSgGZWrr"}},{"cell_type":"code","source":["issues_with_comments_dataset.push_to_hub(\"github-issues\")"],"metadata":{"id":"PO6zuNesZdLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From here, anyone can download the dataset by simply providing `load_dataset()` with the repository ID as the `path` argument:"],"metadata":{"id":"H15T5tW5Zc1Z"}},{"cell_type":"code","source":["remote_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n","remote_dataset"],"metadata":{"id":"2bYApAFLZqqP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There's just one important thing left to do: adding a _dataset card_ that explains how the corpus was created and provides other useful information for the community."],"metadata":{"id":"U-4JojDTZvVU"}},{"cell_type":"markdown","source":["#### **Creating a dataset card**\n","\n","Well-documented datasets are more likely to be useful to others (including your future self!), as they provide the context to enable users to decide whether the dataset is relevant to their task and to evaluate any potential biases in or risks associated with using the dataset.\n","\n","On the Hugging Face Hub, this information is stored in each dataset repository's _README.md_ file. There are two main steps you should take before creating this file:\n","\n","1. Use the [`datasets-tagging` application](https://huggingface.co/datasets/tagging/) to create metadata tags in YAML format. These tags are used for a variety of search features on the Hugging Face Hub and ensure your dataset can be easily found by members of the community. Since we have created a custom dataset here, you'll need to clone the `datasets-tagging` repository and run the application locally.\n","2. Read the [HF Datasets guide](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) on creating informative dataset cards and use it as a template.\n","\n","You can create the _README.md_ file directly on the Hub, and you can find a template dataset card in the `lewtun/github-issues` dataset repository.\n","\n","That's it! We've seen in this section that creating a good dataset can be quite involved, but fortunately uploading it and sharing it with the community is not. In the next section we'll use our new dataset to create a semantic search engine with HF Datasets that can match questions to the most relevant issues and comments."],"metadata":{"id":"B0_LPZY4aETz"}},{"cell_type":"markdown","source":["### **5.5. Semantic search with FAISS**\n","\n","In section 5, we created a dataset of GitHub issues and comments from the HF Datasets repository. In this section we'll use this information to build a search engine that can help us find answers to our most pressing questions about the library!"],"metadata":{"id":"EMBDPhCLQHcl"}},{"cell_type":"markdown","source":["#### **Using embeddings for semantic search**\n","\n","As we saw in chapter 1, Transformer-based language models represent each token in a span of text as an _embedding vector_. It turns out that one can \"pool\" the individual embeddings to create a vector representation for whole sentences, paragraphs, or (in some cases) documents. These embeddings can then be used to find similar documents in the corpus by computing the dot-product similarity (or some other similarity metric) between each embedding and returning the documents with the greatest overlap.\n","\n","In this section we'll use embeddings to develop a semantic search engine. These search engines offer several advantages over conventional approaches that are based on matching keywords in a query with the documents."],"metadata":{"id":"VMjIgtvOfQj2"}},{"cell_type":"markdown","source":["#### **Loading and preparing the dataset**\n","\n","The first thing we need to do is download our dataset of GitHub issues, so let's use `load_dataset()` function as usual:"],"metadata":{"id":"pxtLny-MgDEe"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n","issues_dataset"],"metadata":{"id":"9-6nIMzvgNIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we've specified the default `train` split in `load_dataset()`, so it returns a `Dataset` instead of a `DatasetDict`. The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine. As should be familiar by now, we can use the `Dataset.filter()` function to exclude these rows in our dataset. While we're at it, let's also filter out rows with no comments, since these provide no answers to user queries:"],"metadata":{"id":"MlepbslbgPqq"}},{"cell_type":"code","source":["issues_dataset = issues_dataset.filter(\n","    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",")\n","issues_dataset"],"metadata":{"id":"sG-BJh_Qg5FC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that there are a lot of columns in our dataset, most of which we don't need to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let's use the `Dataset.remove_columns()` function to drop the rest:"],"metadata":{"id":"4tzRUFHhg8tZ"}},{"cell_type":"code","source":["columns = issues_dataset.column_names\n","columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n","columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n","issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n","issues_dataset"],"metadata":{"id":"jfRr2dychWUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To create our embeddings we'll augment each comment with the issue's title and body, since these fields often include useful contextual information. Because our `comments` column is currently a list of comments for each issue, we need to \"explode\" the column so that each row consists of an `(html_url, title, body, comment)` tuple. In Pandas we can do this with the [`DataFrame.explode()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let's first switch to the Pandas `DataFrame` format:"],"metadata":{"id":"ZrzYXmWMhgv-"}},{"cell_type":"code","source":["issues_dataset.set_format(\"pandas\")\n","df = issues_dataset[:]"],"metadata":{"id":"hAUwD-gwiQ16"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we inspect the first row in this `DataFrame` we can see there are multiple comments associated with this issue:"],"metadata":{"id":"GDurBdKwiVjU"}},{"cell_type":"code","source":["df[\"comments\"][0].tolist()"],"metadata":{"id":"yaRNv888ib7I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we explode `df`, we expect to get one row for each of these comments. Let's check if that's the case:"],"metadata":{"id":"TvQFZmyqigbd"}},{"cell_type":"code","source":["comments_df = df.explode(\"comments\", ignore_index=True)\n","comments_df.head(4)"],"metadata":{"id":"bHofz2aHirTz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great, we can see the rows have been replicated, with the `comments` column containing the individual comments! Now that we're finished with Pandas, we can quickly switch back to a `Dataset` by loading the `DataFrame` in memory:"],"metadata":{"id":"hrNHCrK8ixY0"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","comments_dataset = Dataset.from_pandas(comments_df)\n","comments_dataset"],"metadata":{"id":"b9mSw2N-jG4F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, this has given us a few thousand comments to work with! Now that we have one comment per row, let's create a new `comment_length` column that contains the number of words per comment:"],"metadata":{"id":"ZmkesefHjN1J"}},{"cell_type":"code","source":["comments_dataset = comments_dataset.map(\n","    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",")"],"metadata":{"id":"iLTrCTSkji5g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use this new column to filter out short comments, which typically include things like \"cc @lewtun\" or \"Thanks!\" that are not relevant for our search engine. There's no precise number to select for the filter, but around 15 words seems like a good start:"],"metadata":{"id":"k3536Bqajqgn"}},{"cell_type":"code","source":["comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n","comments_dataset"],"metadata":{"id":"ohpXT-CDkILu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Having cleaned up our dataset a bit, let's concatenate the issue title, description, and comments together in a new `text` column. As usual, we'll write a simple function that we can pass to `Dataset.map()`:"],"metadata":{"id":"vpg2W8VdkL2G"}},{"cell_type":"code","source":["def concatenate_text(examples):\n","    return {\n","        \"text\": examples[\"title\"]\n","        + \" \\n \"\n","        + examples[\"body\"]\n","        + \" \\n \"\n","        + examples[\"comments\"]\n","    }\n","\n","\n","comments_dataset = comments_dataset.map(concatenate_text)"],"metadata":{"id":"y8dFte1qkbOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're finally ready to create some embeddings! Let's take a look."],"metadata":{"id":"tClQjd9wkkPa"}},{"cell_type":"markdown","source":["#### **Creating text embeddings**\n","\n","We saw in chapter 2 that we can obtain token embeddings by using the `AutoModel` class. All we need to do is pick a suitable checkpoint to load the model from. Fortunately, there's a library called `sentence-transformers` that is dedicated to creating embeddings. As described in the library's [documentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), our use case is an example of _asymmetric semantic search_ because we have a short query whose answer we'd like to find in a longer document, like an issue comment. The handy [model overview table](https://www.sbert.net/docs/pretrained_models.html#model-overview) in the documentation indicates that the `multi-qa-mpnet-base-dot-v1` checkpoint has the best performance for semantic search, so we'll use that for our application. We'll also load the tokenizer using the same checkpoint:"],"metadata":{"id":"swsXirPMkqSU"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","# TensorFlow: from transformers import AutoTokenizer, TFAutoModel\n","\n","model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = AutoModel.from_pretrained(model_ckpt)\n","# TensorFlow: model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)"],"metadata":{"id":"idhZurmbl1K9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TensorFlow:**\n","\n","Note that we've set `from_pt=True` as an argument of the `from_pretrained()` method. That's because the `multi-qa-mpnet-base-dot-v1` checkpoint only has PyTorch weights, so setting `from_pt=True` will automatically convert them to the TensorFlow format for us. As you can see, it is very simple to switch between frameworks in HF Transformers!\n","\n","**PyTorch:**\n","\n","To speed up the embedding process, it helps to place the model and inputs on a GPU device, so let's do that now:"],"metadata":{"id":"kKHJuQJxl-_9"}},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\")\n","model.to(device)"],"metadata":{"id":"AyQMNf1ZmE1Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Both PyTorch and TensorFlow:**\n","\n","As we mentioned earlier, we'd like to represent each entry in our GitHub issues corpus as a single vector, so we need to \"pool\" or average our token embeddings in some way. One popular approach is to perform _CLS pooling_ on our model's outputs, where we simply collect the last hidden state for the special `[CLS]` token. The following function does the trick for us:"],"metadata":{"id":"1HHg5rBQmJ-T"}},{"cell_type":"code","source":["def cls_pooling(model_output):\n","    return model_output.last_hidden_state[:, 0]"],"metadata":{"id":"9R5IMFQAmrld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"],"metadata":{"id":"txOd4eR_mtl-"}},{"cell_type":"code","source":["def get_embeddings(text_list):\n","    encoded_input = tokenizer(\n","        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n","        # TensorFlow: text_list, padding=True, truncation=True, return_tensors=\"tf\"\n","    )\n","    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","    # TensorFlow: encoded_input = {k: v for k, v in encoded_input.items()}\n","    model_output = model(**encoded_input)\n","    return cls_pooling(model_output)"],"metadata":{"id":"o709LrW8m5HC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape:"],"metadata":{"id":"kJdQWDkgnKnG"}},{"cell_type":"code","source":["embedding = get_embeddings(comments_dataset[\"text\"][0])\n","embedding.shape"],"metadata":{"id":"S78j_8nrnWta"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great, we've converted the first entry in our corpus into a 768-dimensional vector! We can use `Dataset.map()` to apply our `get_embeddings()` function to each row in our corpus, so let's create a new `embeddings` column as follows:"],"metadata":{"id":"h8ufdcPtnYdW"}},{"cell_type":"code","source":["embeddings_dataset = comments_dataset.map(\n","    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n","    # TensorFlow: lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).numpy()[0]}\n",")"],"metadata":{"id":"w0WBpOien2L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that we've converted the embeddings to NumPy arrays - that's because HF Datasets requires this format when we try to index them with FAISS, which we'll do next."],"metadata":{"id":"KEr-BWMdn7xO"}},{"cell_type":"markdown","source":["#### **Using FAISS for efficient similarity search**\n","\n","Now that we have a dataset of embeddings, we need some way to search over them. To do this, we'll use a special data structure in HF Datasets called a _FAISS index_. [FAISS](https://faiss.ai/) (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n","\n","The basic idea behind FAISS is to create a special data structure called an _index_ that allows one to find which embeddings are similar to an input embedding. Creating a FAISS index in HF Datasets is simple - we use the `Dataset.add_faiss_index()` function and specify which column of our dataset we'd like to index:"],"metadata":{"id":"YjcnFcL5oMGM"}},{"cell_type":"code","source":["embeddings_dataset.add_faiss_index(column=\"embeddings\")"],"metadata":{"id":"_0Jw6aGipErK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.get_nearest_examples()` function. Let's test this out by first embedding a question as follows:"],"metadata":{"id":"yTfa3Us9pFme"}},{"cell_type":"code","source":["question = \"How can I load a dataset offline?\"\n","question_embedding = get_embeddings([question]).cpu().detach().numpy()\n","# TensorFlow: question_embedding = get_embeddings([question]).numpy()\n","question_embedding.shape"],"metadata":{"id":"GB1mF6m0pYfB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings:"],"metadata":{"id":"Jk7nVWbtpcNK"}},{"cell_type":"code","source":["scores, samples = embeddings_dataset.get_nearest_examples(\n","    \"embeddings\", question_embedding, k=5\n",")"],"metadata":{"id":"Tknc3BLKpnRZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `Dataset.get_nearest_examples()` function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let's collect these in a `pandas.DataFrame` so we can easily sort them:"],"metadata":{"id":"GskrCRU3ppVR"}},{"cell_type":"code","source":["import pandas as pd\n","\n","samples_df = pd.DataFrame.from_dict(samples)\n","samples_df[\"scores\"] = scores\n","samples_df.sort_values(\"scores\", ascending=False, inplace=True)"],"metadata":{"id":"vS7Sojcap-Hq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can iterate over the first few rows to see how well our query matched the available comments:"],"metadata":{"id":"_O0uoXYLqCx8"}},{"cell_type":"code","source":["for _, row in samples_df.iterrows():\n","    print(f\"COMMENT: {row.comments}\")\n","    print(f\"SCORE: {row.scores}\")\n","    print(f\"TITLE: {row.title}\")\n","    print(f\"URL: {row.html_url}\")\n","    print(\"=\" * 50)\n","    print()"],"metadata":{"id":"HsFXLz0WqO-O"},"execution_count":null,"outputs":[]}]}