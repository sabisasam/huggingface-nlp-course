{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN7nhZJv+U/uAd39SX6/3GV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Hugging Face NLP Course - Chapter 2**\n","\n","Source: https://huggingface.co/learn/nlp-course/chapter2/1\n","\n","---"],"metadata":{"id":"VAV0UYUR8bzb"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"OAqx3n_T_gfp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Using HF Transformers**"],"metadata":{"id":"XN6NSBd98yPm"}},{"cell_type":"markdown","source":["### **2.1. Behind the pipeline**\n","\n","The pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n","\n","![full_nlp_pipeline.svg](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"],"metadata":{"id":"upZVSgeXary4"}},{"cell_type":"markdown","source":["#### **Preprocessing with a tokenizer**\n","\n","Converts the text inputs into numbers that the model can make sense of. The _tokenizer_ is responsible for:\n","- splitting the input into words, subwords, or symbols (like punctuation) that are called _tokens_\n","- mapping each token to an integer (those integers are vocabulary indices, which are typically called _input IDs_)\n","- adding additional inputs that may be useful to the model\n","\n","All this preprocessing needs to be done in exactly the same way as when the model was pretrained. The `AutoTokenizer` class, its `from_pretrained()` method and the checkpoint name of our model can be used to download that information from the [Model Hub](https://huggingface.co/models).\n","\n","Once we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model. The only thing left to do is to convert the list of input IDs to tensors.\n","\n","Transformer models only accept _tensors_ as input. You can think of them as NumPy arrays. To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the `return_tensors` argument (if no type is passed, you will get a list of lists as a result)."],"metadata":{"id":"oS-Jq5EKfTBx"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # Downloads tokenizer\n","raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs = tokenizer(\n","    raw_inputs,  # Can pass one sentence or a list of sentences\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\",  # Specifies tensor type (pt=PyTorch, tf=TensorFlow)\n",")\n","print(inputs)"],"metadata":{"id":"YPdPYddghP9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Going through the model**\n","\n","We can download our pretrained model the same way we did with our tokenizer, with the `AutoModel` class.\n","\n","The class instantiates the architecture that contains only the base Transformer module: given some inputs, it outputs what we'll call _hidden states_, also known as _features_. For each model input, we'll retrieve a high-dimensional vector representing the __contextual understanding of that input by the Transformer model__.\n","\n","The architecture consists of an embeddings layer and subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."],"metadata":{"id":"FzJkTlJVkyD2"}},{"cell_type":"code","source":["from transformers import AutoModel\n","# TensorFlow: from transformers import TFAutoModel\n","\n","model = AutoModel.from_pretrained(checkpoint)  # Instantiates model\n","outputs = model(**inputs)\n","# TensorFlow: outputs = model(inputs)\n","print(outputs.last_hidden_state.shape)  # Prints batch size, sequence length, hidden size (vector dimension)"],"metadata":{"id":"fhymsVF2lT0p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["While the hidden states can be useful on their own, they're usually inputs to another part of the model, known as the _head_. Different tasks could be performed with the same architecture, but each of these tasks will have a different head associated with it. The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension.\n","\n","For our example, we will need a model with a sequence classification head. So, we won't actually use the `AutoModel` class, but `AutoModelForSequenceClassification`. If we then look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label)."],"metadata":{"id":"HlIL9oEfv0Oe"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","# TensorFlow: from transformers import TFAutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)\n","# TensorFlow: outputs = model(inputs)\n","print(outputs.logits.shape)"],"metadata":{"id":"Rprf7uC8yeeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Postprocessing the output**\n","\n","The model's outputs aren't probabilities but _logits_, which are the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer.\n","\n","To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config."],"metadata":{"id":"SJ4d22UEzJjl"}},{"cell_type":"code","source":["import torch\n","# TensorFlow: import tensorflow as tf\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","# TensorFlow: predictions = tf.math.softmax(outputs.logits, axis=-1)\n","print(predictions)\n","print(model.config.id2label)  # Print labels and their positions"],"metadata":{"id":"kqgpD3txz1bf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.2. Models**\n","\n","The `AutoModel` class is handy when you want to instantiate any model from a checkpoint. This class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It's a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n","\n","However, if you know the type of model you want to use, you can use the class that defines its architecture directly."],"metadata":{"id":"TB5dPDFb6PR-"}},{"cell_type":"code","source":["# Example: BERT model\n","from transformers import BertConfig, BertModel\n","# TensorFlow: from transformers import BertConfig, TFBertModel\n","\n","# Building the config\n","config = BertConfig()\n","\n","# Building the model from the config\n","model = BertModel(config)  # Model is randomly initialized (random values)!\n","\n","print(config)  # Config contains many attributes that are used to build the model"],"metadata":{"id":"JG1pduo37P-b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, or reuse models that have already been trained."],"metadata":{"id":"pArA79oZ8r_5"}},{"cell_type":"code","source":["from transformers import BertModel  # Can be replaced with equivalent `AutoModel` class\n","# TensorFlow: from transformers import TFBertModel, TFAutoModel\n","\n","model = BertModel.from_pretrained(\"bert-base-cased\")  # Load already trained Transformer model"],"metadata":{"id":"Vdjl-XUE9K4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results."],"metadata":{"id":"H_w6w02R-DuU"}},{"cell_type":"markdown","source":["#### **Saving methods**\n","\n","A model can be saved using the `save_pretrained()` method, which is analogous to the `from_pretrained()` method. It will save two files to your disk: `config.json` and `pytorch_model.bin` in case of PyTorch (or `tf_model.h5` in case of TensorFlow). The `config.json` file contains the attributes necessary to build the model architecture. It also contains some metadata, such as where the checkpoint originated and what HF Transformers version you were using when you last saved the checkpoint. The `pytorch_model.bin` (or `tf_model.h5`) file is known as the _state dictionary_; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters."],"metadata":{"id":"co2MluLQ-tzN"}},{"cell_type":"code","source":["model.save_pretrained(\"directory_on_my_computer\")"],"metadata":{"id":"f_UI5Do0_ACe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.3. Tokenizers**\n","\n","Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. In NLP tasks, the data that is generally processed is raw text. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. There are a lot of ways to go about this. The goal is to find the most meaningful representation - that is, the one that makes the most sense to the model - and, if possible, the smallest representation.\n","\n","Common examples of tokenization algorithms:\n","- _word-based_\n","  - splits text on spaces (variation: spaces and punctuation)\n","  - cons:\n","    - very large vocabularies (results in heavy models)\n","    - large quantity of out-of-vocabulary tokens\n","    - loss of meaning across very similar words (e.g. \"dog\" and \"dogs\" or \"run\" and \"running\"; model identifies similar words as unrelated because of different IDs; model learns different meanings)\n","- _character-based_\n","  - splits text on characters\n","  - pros:\n","    - small vocabularies (always slimmer than their word-based vocabularies counterparts; include letters, numbers, and special characters)\n","    - very few out-of-vocabulary (unknown) tokens (every word can be built from characters; ability to correctly tokenize misspelled words, rather than discarding them as unknown straight away; vocabularies are more complete than their word-based vocabularies counterparts)\n","  - cons:\n","    - very long sequences (sequences are translated into very large amount of tokens to be processed by the model; can have an impact on the size of the context the model will carry around; reduces the size of the text we can use as input for our model)\n","    - less meaningful individual tokens (characters do not hold as much information individually as a word would hold; true for languages like roman-based languages; not true for all languages, as some languages like ideogram-based languages have a lot of information held in single characters)\n","- _subword-based_\n","  - combines word-based and character-based\n","  - principle: frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords\n","  - pros:\n","    - tokens have a semantic meaning while being space-efficient\n","    - allows us to have a relatively good coverage with small vocabularies, and close to no unknown tokens\n","    - especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords\n","\n","Other examples:\n","- byte-level BPE (used in GPT-2)\n","- WordPiece (used in BERT)\n","- SentencePiece or Unigram (used in several multilingual models)"],"metadata":{"id":"VJpV8EZfDo_V"}},{"cell_type":"markdown","source":["#### **Loading and saving**\n","\n","Loading and saving tokenizers is as simple as it is with models. Actually, it's based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the _architecture_ of the model) as well as its vocabulary (a bit like the _weights_ of the model)."],"metadata":{"id":"M2F1R8EJwwkW"}},{"cell_type":"code","source":["from transformers import BertTokenizer  # Can be replaced with equivalent `AutoTokenizer` class\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")  # Load tokenizer\n","\n","# Do stuff (like fine-tuning, using the tokenizer, ...)\n","\n","tokenizer.save_pretrained(\"directory_on_my_computer\")  # Save tokenizer"],"metadata":{"id":"z-02VSKyvck8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Encoding**\n","\n","Translating text to numbers is known as _encoding_. Encoding is done in a two-step process: the tokenization (splitting text into _tokens_, adding potential special tokens), followed by the conversion to input IDs (converting each token to their unique respective ID as defined by the tokenizer's _vocabulary_).\n","\n","To get a better understanding of the two steps, we'll explore them separately, using appropriate methods. Note that in practice, you should call the tokenizer directly on your inputs (instead of using those methods)."],"metadata":{"id":"qPRfiAFvw7OX"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = \"Using a Transformer network is simple\"\n","tokens = tokenizer.tokenize(sequence)  # Does tokenization process\n","print(tokens)  # Output is list of strings/tokens\n","\n","input_ids = tokenizer.convert_tokens_to_ids(tokens)  # Converts tokens to input IDs\n","print(input_ids)  # List of input IDs, not yet converted to appropriate framework tensor\n","\n","# Perform missing step: adding special tokens needed by the model\n","final_inputs = tokenizer.prepare_for_model(input_ids)  # Adds special tokens\n","print(final_inputs[\"input_ids\"])  # List of input IDs, with IDs of special tokens"],"metadata":{"id":"xuKVlbkZASJl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Decoding**\n","\n","_Decoding_ is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method. Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (e.g. text generation, translation, summarization)."],"metadata":{"id":"ymQTd9vrCIfM"}},{"cell_type":"code","source":["decoded_string = tokenizer.decode(input_ids)  # Decodes input IDs\n","print(decoded_string)  # Should be original input text"],"metadata":{"id":"-FObllMGCVar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.4. Handling multiple sequences**"],"metadata":{"id":"X83BGca2FSRI"}},{"cell_type":"markdown","source":["#### **Models expect a batch of inputs**\n","\n","HF Transformers models expect multiple sentences by default. _Batching_ is the act of sending multiple sentences through the model, all at once. Batching allows the model to work when you feed it multiple sentences."],"metadata":{"id":"4hZpOtf6sVMp"}},{"cell_type":"code","source":["import torch\n","# TensorFlow: import tensorflow as tf\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","# TensorFlow: from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","input_ids = torch.tensor([ids])  # Add dimension to convert it to a batch\n","# TensorFlow: input_ids = tf.constant([ids])\n","print(\"Input IDs:\", input_ids)\n","\n","output = model(input_ids)  # Only works for batches, not for single sequences\n","print(\"Logits:\", output.logits)"],"metadata":{"id":"TonimLoQwHKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But there's an issue. When you're trying to batch together two (or more) sentences, they might be of different lengths. Tensors need to be of rectangular shape, so you won't be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually _pad_ the inputs."],"metadata":{"id":"NBq8guPew-Sa"}},{"cell_type":"markdown","source":["#### **Padding the inputs**\n","\n","As a workaround, we'll use _padding_ to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the _padding token_ to the sentences with fewer values. For example, if you have 9 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. The padding token ID can be found in `tokenizer.pad_token_id`."],"metadata":{"id":"XAEF6i_btAJn"}},{"cell_type":"code","source":["sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)\n","# TensorFlow: print(model(tf.constant(...)).logits)"],"metadata":{"id":"IK_tqK5puV-o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will notice that some logits in our batched predictions differ from the logits of the individual predictions. This is because the key feature of Transformer models is attention layers that _contextualize_ each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different length through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."],"metadata":{"id":"U981D9s6xwjQ"}},{"cell_type":"markdown","source":["#### **Attention masks**\n","\n","_Attention masks_ are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model). We can use attention masks to get the same result for the same sequence, whether it is processed in a batch or individually."],"metadata":{"id":"JyKan7px0XeL"}},{"cell_type":"code","source":["sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)).logits)\n","# TensorFlow: print(model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask)).logits)"],"metadata":{"id":"p7Z1RJsb2AuZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Longer sequences**\n","\n","With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n","- Use a model with a longer supported sequence length.\n","- Truncate your sequences.\n","\n","Models have different supported sequence lengths, and some specialize in handling very long sequences. If you're working on a task that requires very long sequences, we recommend you take a look at such models (e.g. [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer), [LED](https://huggingface.co/docs/transformers/model_doc/led)). Otherwise, we recommend you truncate your sequences by specifying the `max_sequence_length` parameter."],"metadata":{"id":"wEP-gOGK5DGt"}},{"cell_type":"markdown","source":["### **2.5. Putting it all together**\n","\n","In the last few sections, we've been trying our best to do most of the work by hand. We've explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.\n","\n","However, the HF Transformers API can handle all of this for us with a high-level function. When you call your `tokenizer` directly on the sentence, you get back inputs that are ready to pass through your model.\n","\n","Note that some models add special words to sequences. The tokenizer knows which ones are expected and will deal with this for you."],"metadata":{"id":"jJE1e_8b_6_t"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","model_inputs = tokenizer(sequence)  # Contains everything that's necessary for the model to operate well\n","\n","# Also works for multiple sequences\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","model_inputs = tokenizer(sequences)\n","\n","# Padding\n","\n","# Will pad the sequences up to the maximum sequence length\n","model_inputs = tokenizer(sequences, padding=\"longest\")\n","# Will pad the sequences up to the model max length\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","# Will pad the sequences up to the specified max length\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n","\n","# Truncation\n","\n","# Will truncate the sequences that are longer than the model max length\n","model_inputs = tokenizer(sequences, truncation=True)\n","# Will truncate the sequences that are longer than the specified max length\n","model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n","\n","# Conversion to specific framework tensors\n","\n","# Returns PyTorch tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n","# Returns TensorFlow tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n","# Returns NumPy arrays\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n","\n","# All together\n","\n","model_inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")"],"metadata":{"id":"XJxfwBuL-Hby"},"execution_count":null,"outputs":[]}]}