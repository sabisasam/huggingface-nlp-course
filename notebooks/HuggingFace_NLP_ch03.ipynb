{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMIpwhInMIxZjEZppDbL+TO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Hugging Face NLP Course - Chapter 3**\n","\n","Source: https://huggingface.co/learn/nlp-course/chapter3/1\n","\n","---"],"metadata":{"id":"9nZ2p-t_IUeE"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"pEjOdNm2KOaL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Fine-Tuning A Pretrained Model**"],"metadata":{"id":"OqtOtJmBIa6t"}},{"cell_type":"markdown","source":["### **3.1. Processing the data**\n","\n","Here is how we would train a sequence classifier on one batch:"],"metadata":{"id":"1pFlRzfFKnt5"}},{"cell_type":"code","source":["import torch\n","from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n","# TensorFlow:\n","#     import tensorflow as tf\n","#     import numpy as np\n","#     from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequences = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"This course is amazing!\",\n","]\n","\n","# PyTorch:\n","batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","batch[\"labels\"] = torch.tensor([1, 1])\n","\n","optimizer = AdamW(model.parameters())\n","loss = model(**batch).loss\n","loss.backward()\n","optimizer.step()\n","\n","# TensorFlow:\n","#     batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n","#\n","#     model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n","#     labels = tf.convert_to_tensor([1, 1])\n","#     models.train_on_batch(batch, labels)"],"metadata":{"id":"SyJ8km0r3b6Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset."],"metadata":{"id":"Xrg_qM8X56DP"}},{"cell_type":"markdown","source":["#### **Loading a dataset from the Hub**\n","\n","The Hub doesn't just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets [here](https://huggingface.co/datasets) or see the general documentation [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub).\n","\n","We will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, which consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). The dataset is one of the 10 datasets composing the [GLUE benchmark](https://gluebenchmark.com/), which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.\n","\n","The HF Datasets library provides a very simple command to download and cache a dataset on the Hub, `load_dataset`."],"metadata":{"id":"jz2RhsgWCsIc"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","raw_datasets"],"metadata":{"id":"RORsFJu7EpeO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, we get a `DatasetDict` object which contains the training set, the validation set, and the test set. Each of those contains several columns (`sentence1`, `sentence2`, `label`, and `idx`) and a variable number of rows, which are the number of elements in each set.\n","\n","This command downloads and caches the dataset, by default in _~/.cache/huggingface/datasets_. You can customize your cache folder by setting the `HF_HOME` environment variable.\n","\n","We can access each pair of sentences in our `raw_datasets` object by indexing, like with a dictionary."],"metadata":{"id":"MllfV4O-E4M7"}},{"cell_type":"code","source":["raw_train_dataset = raw_datasets[\"train\"]\n","raw_train_dataset[0]"],"metadata":{"id":"2t1iyUR6Fz56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see the labels are already integers, so we won't have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the `features` of our `raw_train_dataset`. This will tell us the type of each column. Behind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name is stored in the _names_ folder. Here, `0` corresponds to `not_equivalent`, and `1` corresponds to `equivalent`."],"metadata":{"id":"VMvuXzJFF9bC"}},{"cell_type":"code","source":["raw_train_dataset.features"],"metadata":{"id":"-HYvLkcKGP6z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Preprocessing a dataset**\n","\n","To preprocess the dataset, we need to convert the text to numbers the model can make sense of. This is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences. However, we can't just pass two tokenized sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our model expects."],"metadata":{"id":"lq_cBbALHoBQ"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n","inputs"],"metadata":{"id":"7_1b9Kb7-BCJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this example, the `token_type_ids` tell the model which part of the input is the first sentence and which is the second sentence.\n","\n","Note that if you select a different checkpoint, you won't necessarily have the `token_type_ids` in your tokenized inputs. They are only returned when the model will know what to do with them, because it has seen them during its pretraining. In general, you don't need to worry about whether or not there are `token_type_ids` in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.\n","\n","On top of the masked language modeling objective, BERT has an additional objective called _next sentence prediction_. The goal with this task is to model the relationship between pairs of sentences. With next sentence prediction, the model is provided pairs of sentences (with randomly masked tokens) and asked to predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents.\n","\n","Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options."],"metadata":{"id":"LzUQnl2t-QmQ"}},{"cell_type":"code","source":["tokenized_dataset = tokenizer(\n","    raw_datasets[\"train\"][\"sentence1\"],\n","    raw_datasets[\"train\"][\"sentence2\"],\n","    padding=True,\n","    truncation=True,\n",")"],"metadata":{"id":"aKZO5nEJDL8j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This works well, but is has the disadvantage of returning a dictionary. It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the HF Datasets library are [Apache Arrow](https://arrow.apache.org/) files stored on the disk, so you only keep the samples you ask for loaded in memory).\n","\n","To keep the data as a dataset, we will use the [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The `map()` method works by applying a function on each element of the dataset."],"metadata":{"id":"SsQWiKNODaj0"}},{"cell_type":"code","source":["# Function that tokenizes our inputs\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"],"metadata":{"id":"ZjJMfbUVErr3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that the function also works if the `example` dictionary contains several samples (each key as a list of sentences) since the `tokenizer` works on lists of pairs of sentences, as seen before. This will allow us to use the option `batched=True` in our call to `map()`, which will greatly speed up the tokenization. The `tokenizer` is backed by a tokenizer written in Rust from the [HF Tokenizers](https://github.com/huggingface/tokenizers) library. This tokenizer can be very fast, but only if we give it lots of inputs at once.\n","\n","Note that we've left the `padding` argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it's better to pad the samples when we're building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!\n","\n","You can even use multiprocessing when applying your preprocessing function with `map()` by passing along a `num_proc` argument. We didn't do this here because the HF Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing."],"metadata":{"id":"nvQpdKzTE_U2"}},{"cell_type":"code","source":["# Apply function on all our datasets at once\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","# batched=True -> function is applied to multiple elements of our dataset at once,\n","#                 not on each element separately (allows for faster preprocessing)\n","tokenized_datasets"],"metadata":{"id":"I2MBRNreGmJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The way the HF Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function. Our `tokenize_function` returns a dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`, so those three fields are added to all splits of our dataset. Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied `map()`.\n","\n","The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together - a technique we refer to as _dynamic padding_. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept."],"metadata":{"id":"rcLqyEv5H-DW"}},{"cell_type":"markdown","source":["#### **Dynamic padding**\n","\n","The function that is responsible for putting together samples inside a batch is called a _collate function_. In case of PyTorch, it's an argument you can pass when you build a `DataLoader`.\n","\n","We only want to apply padding as necessary on each batch instead of the whole dataset to avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems - TPUs prefer fixed shapes, even when that requires extra padding.\n","\n","To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the HF Transformers library provides us with such a function via `DataCollatorWithPadding`. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need."],"metadata":{"id":"I3iEy-f_MFlS"}},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","# TensorFlow: data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n","\n","samples = tokenized_datasets[\"train\"][:8]  # Grab samples that we would like to batch together\n","samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}  # Remove columns we don't need\n","print([len(x) for x in samples[\"input_ids\"]])  # Print lengths of each entry in the batch\n","\n","batch = data_collator(samples)\n","print({k: v.shape for k, v in batch.items()})  # Check if data_collator is dynamically padding the batch properly"],"metadata":{"id":"dUzlMwfzThTb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PyTorch:**\n","\n","Now that we've gone from raw text to batches our model can deal with, we're ready to fine-tune it (section 1.2.)!\n","\n","**TensorFlow:**\n","\n","Now that we have our dataset and a data collator, we need to put them together. We could manually load batches and collate them, but that's a lot of work, and probably not very performant either. Instead, there's a simple method that offers a performant solution to this problem: `to_tf_dataset()`. This will wrap a `tf.data.Dataset` around your dataset, with an optional collation function. `tf.data.Dataset` is a native TensorFlow format that Keras can use for `model.fit()`, so this one method immediately converts a HF Dataset to a format that's ready for training."],"metadata":{"id":"1cPO2vkuYOPH"}},{"cell_type":"code","source":["# Only necessary for TensorFlow\n","\n","tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")"],"metadata":{"id":"2B7c0qPGcIue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can take those datasets forward into the next lecture, where training will be pleasantly straightforward after all the hard work of data preprocessing."],"metadata":{"id":"GnzmAts5cPlD"}},{"cell_type":"markdown","source":["### **3.2. Fine-tuning a model with the Trainer API or Keras**"],"metadata":{"id":"ifGFO8Ssc5Yu"}},{"cell_type":"markdown","source":["#### **3.2.1. Fine-tuning a model with the Trainer API (PyTorch)**\n","\n","HF Transformers provides a `Trainer` class to help you fine-tune any of the pretrained models it provides on your dataset. Once you've done all the data preprocessing work in the last section, you have just a few steps left to define the `Trainer`. The hardest part is likely to be preparing the environment to run `Trainer.train()`, as it will run very slowly on a CPU."],"metadata":{"id":"5lg_V7q4_6d_"}},{"cell_type":"code","source":["# What we need from the previous section for the code examples below\n","\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"EVED72ODENUX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Training**\n","\n","The first step before we can define our `Trainer` is to define a `TrainingArguments` class that will contain all the hyperparameters the `Trainer` will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning. Hint: If you want to automatically upload your model to the Hub during training, pass along `push_to_hub=True` in the `TrainingArguments`.\n","\n","The second step is to define our model. As in the previous chapter, we will use the `AutoModelForSequenceClassification` class, with two labels."],"metadata":{"id":"bX-NQ05PF0TM"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, TrainingArguments\n","\n","training_args = TrainingArguments(\"test-trainer\")  # First step\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)  # Second step"],"metadata":{"id":"kn_T23b7GQCC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will notice that unlike in chapter 2, you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.\n","\n","Once we have our model, we can define a `Trainer` by passing it all the objects constructed up to now - the `model`, the `training_args`, the training and validation datasets, our `data_collator`, and our `tokenizer`."],"metadata":{"id":"jcx4nr5MGtOd"}},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,  # This line can be removed in this case\n","    tokenizer=tokenizer,\n",")"],"metadata":{"id":"srAtL9PgMnHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that when you pass the `tokenizer` as we did here, the default `data_collator` used by the `Trainer` will be a `DataCollatorWithPadding` as defined previously, so you can skip the line `data_collator=data_collator` in this call. It was still important to show you this part of the processing in section 2!\n","\n","To fine-tune the model on our dataset, we just have to call the `train()` method of our Trainer."],"metadata":{"id":"HXpRampqNCjY"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"Rio27Xs3N3Mj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It won't, however, tell you how well (or badly) your model is performing. This is because:\n","\n","1. We didn't tell the `Trainer` to evaluate during training by setting `evaluation_strategy` to either `\"steps\"` (evaluate every `eval_steps`) or `\"epoch\"` (evaluate at the end of each epoch).\n","2. We didn't provide the `Trainer` with a `compute_metrics()` function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number)."],"metadata":{"id":"Qh32RGAqOAd9"}},{"cell_type":"markdown","source":["##### **Evaluation**\n","\n","Let's see how we can build a useful `compute_metrics()` function and use it the next time we train. The function must take an `EvalPrediction` object (which is a named tuple with a `predictions` field and a `label_ids` field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the `Trainer.predict()` command."],"metadata":{"id":"upubg0zsPLMQ"}},{"cell_type":"code","source":["predictions = trainer.predict(tokenized_datasets[\"validation\"])\n","print(predictions.predictions.shape, predictions.label_ids.shape)"],"metadata":{"id":"yh-lUsK7P3wS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The output of the `predict()` method is another named tuple with three fields: `predictions`, `label_ids`, and `metrics`. The `metrics` field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our `compute_metrics()` function and pass it to the `Trainer`, that field will also contain the metrics returned by `compute_metrics()`.\n","\n","As you can see, in this case `predictions` is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to `predict()` (as you saw in the previous chapter, all Transformer models return logits). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis."],"metadata":{"id":"VxFnkO_gQbTP"}},{"cell_type":"code","source":["import numpy as np\n","\n","preds = np.argmax(predictions.predictions, axis=-1)"],"metadata":{"id":"8d5Jd85dR20Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now compare those `preds` to the labels. To build our `compute_metrics()` function, we will rely on the metrics from the [HF Evaluate](https://github.com/huggingface/evaluate/) library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `evaluate.load()` function. The object returned has a `compute()` method we can use to do the metric calculation."],"metadata":{"id":"SpfeAXg0SBtv"}},{"cell_type":"code","source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=preds, references=predictions.label_ids)"],"metadata":{"id":"3vheLXomSps1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. The accuracy and F1 score (which get returned above) are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark.\n","\n","Wrapping everything together, we get our `compute_metrics()` function."],"metadata":{"id":"u4gQpOZFS2fj"}},{"cell_type":"code","source":["def compute_metrics(eval_preds):\n","    metric = evaluate.load(\"glue\", \"mrpc\")\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"id":"iEsz1sf5U0Px"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And to see it used in action to report metrics at the end of each epoch, here is how we define a new `Trainer` with this `compute_metrics()` function."],"metadata":{"id":"1QeOAqkoVMVV"}},{"cell_type":"code","source":["training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()  # Launch new training run"],"metadata":{"id":"ghvCslg6VTD3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we create a new `TrainingArguments` with its `evaluation_strategy` set to `\"epoch\"` and a new model - otherwise, we would just be continuing the training of the model we have already trained.\n","\n","This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss.\n","\n","The `Trainer` will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use `fp16 = True` in your training arguments). We will go over everything it supports in chapter 10.\n","\n","This concludes the introduction to fine-tuning using the `Trainer` API. An example of doing this for most common NLP tasks will be given in chapter 7, but for now let's look at how to do the same thing in pure PyTorch."],"metadata":{"id":"E7DV0MxoWr4H"}},{"cell_type":"markdown","source":["#### **3.2.2. Fine-tuning a model with Keras (TensorFlow)**\n","\n","https://huggingface.co/learn/nlp-course/chapter3/3?fw=tf\n","\n","Once you've done all the data preprocessing work in the last section, you have just a few steps left to train the model. Note, however, that the `model.fit()` command will run very slowly on a CPU."],"metadata":{"id":"eJc45doRAGhA"}},{"cell_type":"code","source":["# What we need from the previous section for the code examples below\n","\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","import numpy as np\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n","\n","tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")"],"metadata":{"id":"sw0U75BVknWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Training**\n","\n","TensorFlow models imported from HF Transformers are already Keras models. That means that once we have our data, very little work is required to begin training on it.\n","\n","As in the previous chapter, we will use the `TFAutoModelForSequenceClassification` class, with two labels."],"metadata":{"id":"xjBgi6XAklbM"}},{"cell_type":"code","source":["from transformers import TFAutoModelForSequenceClassification\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"],"metadata":{"id":"PlmtFTiC1w2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will notice that unlike in chapter 2, you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been inserted instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.\n","\n","To fine-tune the model on our dataset, we just have to `compile()` our model and then pass our data to the `fit()` method. This will start the fine-tuning process (which should take a couple of minutes on a GPU) and report training loss as it goes, plus the validation loss at the end of each epoch.\n","\n","Note that HF Transformers models have a special ability that most Keras models don't - they can automatically use an appropriate loss which they compute internally. They will use this loss by default if you don't set a loss argument in `compile()`. Note that to use the internal loss you'll need to pass your labels as part of the input, not as a separate label, which is the normal way to use labels with Keras models."],"metadata":{"id":"NMHlPt9n17im"}},{"cell_type":"code","source":["from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=SparseCategoricalCrossentropy)from_logits=True),\n","    metrics=[\"accuracy\"],\n",")\n","model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_validation_dataset,\n",")"],"metadata":{"id":"mJ55eon-F5Dx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note a very common pitfall here - you _can_ just pass the name of the loss as a string to Keras, but by default Keras will assume that you have already applied a softmax to your outputs. Many models, however, output the values right before the softmax is applied, which are also known as the _logits_. We need to tell the loss function that that's what our model does, and the only way to do that is to call it directly, rather than by name with a string."],"metadata":{"id":"5sQ4E89_GWyI"}},{"cell_type":"markdown","source":["##### **Improving training performance**\n","\n","If you try the above code, it certainly runs, but you'll find that the loss declines only slowly or sporadically. The primary cause is the _learning rate_. As with the loss, when we pass Keras the name of an optimizer as a string, Keras initializes that optimizer with default values for all parameters, including learning rate. From long experience, though, we know that transformer models benefit from a much lower learning rate than the default for Adam, which is 1e-3, also written as 10 to the power of -3, or 0.001. 5e-5 (0.00005), which is some twenty times lower, is a much better starting point.\n","\n","In addition to lowering the (constant) learning rate, we can slowly reduce the learning rate over the course of training. In the literature, you will sometimes see this referred to as _decaying_ or _annealing_ the learning rate. In Keras, the best way to do this is to use a _learning rate scheduler_. A good one to use is `PolynomialDecay` - despite the name, with default settings it simply linearly decays the learning rate from the initial value to the final value over the course of training, which is exactly what we want. In order to use a scheduler correctly, though, we need to tell it how long training is going to be. We compute that as `num_train_steps` below."],"metadata":{"id":"eooV9AE0G5-q"}},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers.schedules import PolynomialDecay\n","\n","batch_size = 8\n","num_epochs = 3\n","# The number of training steps is the number of samples in the dataset,\n","# divided by the batch size then multiplied by the total number of epochs.\n","# Note that the tf_train_dataset here is a batched tf.data.Dataset, not the\n","# original HF Dataset, so its len() is already num_samples // batch_size.\n","num_train_steps = len(tf_train_dataset) * num_epochs\n","lr_scheduler = PolynomialDecay(\n","    initial_learning_rate=5e-5,\n","    end_learning_rate=0.0,\n","    decay_steps=num_training_steps,\n",")\n","\n","opt = Adam(learning_rate=lr_scheduler)"],"metadata":{"id":"2LY9ej392rPF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hint: The HF Transformers library also has a `create_optimizer()` function that will create an `AdamW` optimizer with learning rate decay. This is a convenient shortcut.\n","\n","Now we have our all-new optimizer, and we can try training with it. First, let's reload the model, to reset the changes to the weights from the training run we just did, and then we can compile and fit it with the new optimizer."],"metadata":{"id":"J5p0SMvC4R6_"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n","\n","model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"],"metadata":{"id":"bhFyb-ku5ScM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Model predictions**\n","\n","Training and watching the loss go down is all very nice, but what if we want to actually get outputs from the trained model, either to compute some metrics, or to use the model in production? To do that, we can just use the `predict()` method. This will return the _logits_ from the output head of the model, one per class. We can convert these logits into the model's class predictions by using `argmax` to find the highest logit, which corresponds to the most likely class."],"metadata":{"id":"ZTgIDQXd6dge"}},{"cell_type":"code","source":["preds = model.predict(tf_validation_dataset)[\"logits\"]\n","class_preds = np.argmax(preds, axis=1)\n","print(preds.shape, class_preds.shape)"],"metadata":{"id":"4JVhtqw89XHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's use those `preds` to compute some metrics! We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `evaluate.load()` function. The object returned has a `compute()` method we can use to do the metric calculation."],"metadata":{"id":"77Qbg5cl9qWz"}},{"cell_type":"code","source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=class_preds, references=raw_datasets[\"validation\"][\"label\"])"],"metadata":{"id":"vm4FUHJ99-iM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The accuracy and the F1 score are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark.\n","\n","This concludes the introduction to fine-tuning using the Keras API. An example of doing this for most common NLP tasks will be given in chapter 7."],"metadata":{"id":"BI_Rcs69-N_M"}},{"cell_type":"markdown","source":["### **3.3. A full training (PyTorch only)**\n","\n","Now we'll see how to achieve the same results as we did in the last section without using the `Trainer` class. Here is a short summary covering everything you will need:"],"metadata":{"id":"JAZRf5JT_W1p"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_colator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"eOfy5YvyDapb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Prepare for training**\n","\n","Before actually writing our training loop, we will need to define a few objects. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our `tokenized_datasets`, to take care of some things that the `Trainer` did for us automatically. Specifically, we need to:\n","- Remove the columns corresponding to values the model does not expect (like the `sentence1` and `sentence2` columns).\n","- Rename the column `label` to `labels` (because the model expects the argument to be named `labels`).\n","- Set the format of the datasets so they return PyTorch tensors instead of lists.\n","\n","Our `tokenized_datasets` has one method for each of those steps."],"metadata":{"id":"nQcO_wbQENLB"}},{"cell_type":"code","source":["tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","# Check that the result only has columns that our model will accept\n","tokenized_datasets[\"train\"].column_names"],"metadata":{"id":"hqGtYnjaFOc8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that this is done, we can easily define our dataloaders."],"metadata":{"id":"GyYvXx0fFr1Q"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets[\"train\"],\n","    shuffle=True,\n","    batch_size=8,\n","    collate_fn=data_collator,\n",")\n","eval_dataloader = DataLoader(\n","    tokenized_datasets[\"validation\"],\n","    batch_size=8,\n","    collate_fn=data_collator,\n",")\n","\n","# To quickly check there is no mistake in the data processing, we can inspect a batch like this:\n","for batch in train_dataloader:\n","    break\n","{k: v.shape for k, v in batch.items()}"],"metadata":{"id":"ODyyuIfzFx_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we're completely finished with data preprocessing (a satisfying yet elusive goal for any ML practitioner), let's turn to the model. We instantiate it exactly as we did in the previous section."],"metadata":{"id":"Ef9wDbdTG1_5"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","# To make sure that everything will go smoothly during training, we pass our batch to this model:\n","outputs = model(**batch)\n","print(outputs.loss, outputs.logits.shape)"],"metadata":{"id":"t0CwJ7MWHQuS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All HF Transformers models will return the loss when `labels` are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2).\n","\n","We're almost ready to write our training loop! We're just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the `Trainer` was doing by hand, we will use the same defaults. The optimizer used by the `Trainer` is `AdamW`, which is the same as Adam, but with a twist for weight decay regularization."],"metadata":{"id":"O0QwkyXXHrGH"}},{"cell_type":"code","source":["from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"acy51m51IVLt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The `Trainer` uses three epochs by default, so we will follow that."],"metadata":{"id":"l1fLNG-TIcaB"}},{"cell_type":"code","source":["from transformers import get_scheduler\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","print(num_training_steps)"],"metadata":{"id":"lPlS-C1JJjMr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **The training loop**\n","\n","One last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a `device` we will put our model and our batches on."],"metadata":{"id":"1s-vJeM4KA2e"}},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","device"],"metadata":{"id":"tagduFvTKUaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are now ready to train! To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the `tqdm` library."],"metadata":{"id":"EXEDA-ObKjkd"}},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"],"metadata":{"id":"Od8Z54IqKuuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We didn't ask for any reporting, so this training loop will not tell us anything about how the model fares. We need to add an evaluation loop for that."],"metadata":{"id":"26Ui27qpLXK0"}},{"cell_type":"markdown","source":["#### **The evaluation loop**\n","\n","As we did earlier, we will use a metric provided by the HF Evaluate library. We've already seen the `metric.compute()` method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method `add_batch()`. Once we have accumulated all the batches, we can get the final result with `metric.compute()`. Here's how to implement all of this in an evaluation loop."],"metadata":{"id":"3IYwjm-CLvJB"}},{"cell_type":"code","source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","model.eval()\n","for batch in eval_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"],"metadata":{"id":"O8E5ZzGAMMbb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Supercharge your training loop with HF Accelerate**\n","\n","The training loop we defined earlier works fine on a single CPU or GPU. But using the [HF Accelerate](https://github.com/huggingface/accelerate) library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. Starting from the creation of the training and validation dataloaders, here is what our manual training loop looks like (without HF Accelerate):"],"metadata":{"id":"_sgSom_qM5S_"}},{"cell_type":"code","source":["from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"],"metadata":{"id":"A4Kixp45PgxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And here are the changes (with HF Accelerate):"],"metadata":{"id":"WvXgWPfwRDw0"}},{"cell_type":"code","source":["from accelerate import Accelerator  # New line\n","from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n","\n","accelerator = Accelerator()  # New line\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","# Removed line: device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# Removed line: model.to(device)\n","\n","train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n","    train_dataloader, eval_dataloader, model, optimizer,\n",")  # New lines\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        # Removed line: batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        # Removed line: loss.backward()\n","        accelerator.backward(loss)  # New line\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"],"metadata":{"id":"Ck74WvVSRZ0M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The newly instantiated `Accelerator` object will look at the environment and initialize the proper distributed setup. HF Accelerate handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use `accelerator.device` instead of `device`).\n","\n","Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimizer to `accelerator.prepare()`. This will wrap those objects in the proper container to make sure your distributed training works as intended.\n","\n","Putting the above code snippet (you may remove the comments) in a `train.py` script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command `$ accelerate config` which will prompt you to answer a few questions and dump your answers in a configuration file used by the command `$ accelerate launch train.py` which will launch the distributed training.\n","\n","If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a `training_function()` and run a last cell with:"],"metadata":{"id":"OtyVZ7xzSdp0"}},{"cell_type":"code","source":["from accelerate import notebook_launcher\n","\n","notebook_launcher(training_function)"],"metadata":{"id":"GXVY-N5SWDcE"},"execution_count":null,"outputs":[]}]}